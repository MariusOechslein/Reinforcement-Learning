{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gambler's problem (p. 84 in book)\n",
    "- flipping coin (with probability p landing heads)\n",
    "- player can place bet on coin toss (only whole euros)\n",
    "- player wins when reaching 100€\n",
    "- player loses when reaching 0€"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- undiscounted, episodic, finite MDP\n",
    "- state is gamblers capital (1-99)\n",
    "- actions are the stakes placed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "1. Implement the game (coin toss with probability, placing bets, having money)\n",
    "2. Implement MDP with states and possible actions (with plotting functions)\n",
    "3. Try changing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 50 bet: 10\n",
      "you won! 60\n",
      "state: 60 bet: 10\n",
      "you won! 70\n",
      "state: 70 bet: 10\n",
      "you won! 80\n",
      "state: 80 bet: 10\n",
      "you won! 90\n",
      "state: 90 bet: 10\n",
      "you lost! 80\n",
      "state: 80 bet: 10\n",
      "you won! 90\n",
      "state: 90 bet: 10\n",
      "you lost! 80\n",
      "state: 80 bet: 10\n",
      "you lost! 70\n",
      "state: 70 bet: 10\n",
      "you lost! 60\n",
      "state: 60 bet: 10\n",
      "you lost! 50\n",
      "state: 50 bet: 10\n",
      "you won! 60\n",
      "state: 60 bet: 10\n",
      "you won! 70\n",
      "state: 70 bet: 20\n",
      "you lost! 50\n",
      "state: 50 bet: 20\n",
      "you won! 70\n",
      "state: 70 bet: 20\n",
      "you won! 90\n",
      "state: 90 bet: 2\n",
      "you lost! 88\n",
      "state: 88 bet: 20\n",
      "you won! 108\n",
      "Done playing, money: 108\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "PH = 0.4 # probability for heads\n",
    "if PH < 0.5:\n",
    "    PH = 1.0 - PH # we always choose the more probable side\n",
    "\n",
    "class gambler:\n",
    "    WINNING_THRESHOLD = 100\n",
    "    LOSING_THRESHOLD = 0\n",
    "    money = 50\n",
    "\n",
    "    def place_bet(self):\n",
    "        # TODO: Calculate which bet should be placed\n",
    "        bet = int(input(\"Place bet:\"))\n",
    "        print(\"state:\", self.money, \"bet:\", bet)\n",
    "        return bet\n",
    "\n",
    "while gambler.money > gambler.LOSING_THRESHOLD and gambler.money < gambler.WINNING_THRESHOLD:\n",
    "    bet = gambler.place_bet()\n",
    "    if bet < 0 or bet > gambler.money:\n",
    "        print(\"Bet must be between 0 and your amount of money\")\n",
    "        continue\n",
    "\n",
    "    if random.uniform(0,1) <= PH:\n",
    "        gambler.money += bet\n",
    "        print(\"you won!\", gambler.money)\n",
    "    else:\n",
    "        gambler.money -= bet\n",
    "        print(\"you lost!\", gambler.money)\n",
    "print(\"Done playing, money:\", gambler.money)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self):\n",
    "        # Step 1: Define the state space\n",
    "        self.state_space = ...\n",
    "        # Step 2: Define the action space\n",
    "        self.action_space = ...\n",
    "        # Step 3: Define the transition probabilities\n",
    "        self.transition_probs = ...\n",
    "        # Step 4: Define the reward function\n",
    "        self.reward_function = ...\n",
    "\n",
    "    def value_iteration(self):\n",
    "        # Step 5: Implement the value iteration algorithm\n",
    "        value_function = ...\n",
    "        while not convergence:\n",
    "            # Value function update step\n",
    "            value_function_new = ...\n",
    "            # Check for convergence\n",
    "            convergence = ...\n",
    "            # Update value function\n",
    "            value_function = value_function_new\n",
    "\n",
    "        return value_function\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        # Step 5: Implement the policy iteration algorithm\n",
    "        policy = ...\n",
    "        while not convergence:\n",
    "            # Policy evaluation step\n",
    "            value_function = self.policy_evaluation(policy)\n",
    "            # Policy improvement step\n",
    "            new_policy = ...\n",
    "            # Check for convergence\n",
    "            convergence = ...\n",
    "            # Update policy\n",
    "            policy = new_policy\n",
    "        return policy\n",
    "\n",
    "    def policy_evaluation(self, policy):\n",
    "        # Step 5: Implement the policy evaluation algorithm\n",
    "        value_function = ...\n",
    "        while not convergence:\n",
    "            # Value function update step\n",
    "            value_function_new = ...\n",
    "            # Check for convergence\n",
    "            convergence = ...\n",
    "            # Update value function\n",
    "            value_function = value_function_new\n",
    "        return value_function\n",
    "\n",
    "    def execute_policy(self, policy):\n",
    "        # Step 6: Implement the policy execution\n",
    "        state = initial_state\n",
    "        while not episode_terminated:\n",
    "            # Take action based on policy\n",
    "            action = policy[state]\n",
    "            # Update state based on action and transition probabilities\n",
    "            new_state = ...\n",
    "            # Update episode termination condition\n",
    "            episode_terminated = ...\n",
    "            # Update state for next iteration\n",
    "            state = new_state\n",
    "        return final_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 11\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_states = 100\n",
    "epsilon_threshold = 1.0e-5\n",
    "probability_heads = 0.4\n",
    "\n",
    "states = list(range(0, num_states + 1))\n",
    "value_func = np.zeros(num_states + 1)\n",
    "policy = [0 for _ in range(num_states + 1)]\n",
    "reward_func = np.zeros(num_states + 1)\n",
    "reward_func[100] = 1.0 # Reward 1.0 when reaching 100 euros\n",
    "\n",
    "num_iterations = 1\n",
    "while True:\n",
    "    policy_improvement_history = []\n",
    "    # going through all states\n",
    "    for state_index in (range(1, num_states)):\n",
    "        # keeping track of old state value to be able to compare the old value with the new value for each iteration\n",
    "        old_value = value_func[state_index]\n",
    "        optimal_value_temp = value_func[state_index]\n",
    "\n",
    "\n",
    "        for index_new in range(min(state_index, num_states-state_index)+1):\n",
    "            # value iteration equation\n",
    "            new_value = probability_heads * (reward_func[state_index + index_new] + value_func[state_index + index_new]) \\\n",
    "                     + (1 - probability_heads) * (reward_func[state_index - index_new] + value_func[state_index - index_new])\n",
    "\n",
    "            # Change action only if improvement is large enough\n",
    "            if new_value > (optimal_value_temp + epsilon_threshold):\n",
    "                optimal_value_temp = new_value\n",
    "                value_func[state_index] = new_value\n",
    "                policy[state_index] = index_new\n",
    "\n",
    "        # keep track of policy improvement\n",
    "        policy_improvement_this_state = abs(value_func[state_index] - old_value)\n",
    "        policy_improvement_history.append(policy_improvement_this_state)\n",
    "\n",
    "    # Stop iteration if improvement is not large enough\n",
    "    if (max(policy_improvement_history) < epsilon_threshold):\n",
    "        break\n",
    "    num_iterations += 1\n",
    "\n",
    "print(\"Number of iterations:\", num_iterations)\n",
    "plt.bar(states, policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/yz/qr1p847j6qg7rqcz1kxb6gt00000gn/T/ipykernel_92928/784324653.py\", line -1, in <module>\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/mariusoechslein/opt/anaconda3/envs/master/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_states = 100\n",
    "epsilon_threshold = 1.0e-5\n",
    "probability_heads = 0.4\n",
    "\n",
    "states = list(range(0, num_states + 1))\n",
    "value_func = np.zeros(num_states + 1)\n",
    "policy = [0 for _ in range(num_states + 1)]\n",
    "reward_func = np.zeros(num_states + 1)\n",
    "reward_func[100] = 1.0  # Reward 1.0 when reaching 100 euros\n",
    "\n",
    "num_iterations = 1\n",
    "while True:\n",
    "    policy_stable = True\n",
    "    \n",
    "    # Policy Evaluation\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state_index in range(1, num_states):\n",
    "            old_value = value_func[state_index]\n",
    "            bet_index = policy[state_index]\n",
    "\n",
    "            new_value = probability_heads * (reward_func[state_index + bet_index] + value_func[state_index + bet_index]) \\\n",
    "                     + (1 - probability_heads) * (reward_func[state_index - bet_index] + value_func[state_index - bet_index])\n",
    "            value_func[state_index] = new_value\n",
    "\n",
    "            delta = max(delta, abs(old_value - new_value))\n",
    "\n",
    "        if delta < epsilon_threshold:\n",
    "            break\n",
    "\n",
    "    # Policy Improvement\n",
    "    for state_index in range(1, num_states):\n",
    "        old_policy = policy[state_index]\n",
    "        optimal_value_temp = value_func[state_index]\n",
    "        best_bet = 0\n",
    "\n",
    "        num_possible_bets = min(state_index + 1, num_states - state_index + 1)\n",
    "        for bet_index in range(num_possible_bets):\n",
    "            new_value = probability_heads * (reward_func[state_index + bet_index] + value_func[state_index + bet_index]) \\\n",
    "                     + (1 - probability_heads) * (reward_func[state_index - bet_index] + value_func[state_index - bet_index])\n",
    "\n",
    "            if new_value > optimal_value_temp:\n",
    "                optimal_value_temp = new_value\n",
    "                best_bet = bet_index\n",
    "\n",
    "        policy[state_index] = best_bet\n",
    "\n",
    "        if old_policy != best_bet:\n",
    "            policy_stable = False\n",
    "\n",
    "    if policy_stable:\n",
    "        break\n",
    "\n",
    "    num_iterations += 1\n",
    "\n",
    "print(\"Number of iterations:\", num_iterations)\n",
    "plt.bar(states, policy)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
