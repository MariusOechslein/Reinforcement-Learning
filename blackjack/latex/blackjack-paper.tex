\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Deep Reinforcement Learning for Blackjack}

\author{
    \IEEEauthorblockN{Marius Oechslein}
    \IEEEauthorblockA{
        \textit{Faculty of Computer Science and Business Information Systems} \\
        \textit{University of Applied Sciences Würzburg-Schweinfurt}\\
        Würzburg, Germany \\
        marius.oechslein@study.thws.de
    }
}
\maketitle

% TODO: Main focus of paper:
% . Zuerst implementiere ich "traditional RL-methods" (Sarsa, Q-Learning, TD) als Baseline? 
% . Dann implementiere ich Deep Reinforcement Learning und versuche die Baseline zu erreichen.
% . Dann erhöhe ich den state-space durch das genaue counting aller Karten 
% . -> Hierbei möchte ich vergleichen, wie sich DQN im Vergleich zu traditionellen RL-Methoden verhält. 
% .		Ich habe 3 traditionelle, weil der Test dadurch allgemeingültiger ist und nicht von der Implementierungsdetails einer einzelnen Methode abhängt.


\begin{abstract}
\end{abstract}

\begin{IEEEkeywords}
	Blackjack, Reinforcement Learning, Monte Carlo
\end{IEEEkeywords}

\section{Introduction}
\subsection{Blackjack and Reinforcement Learning} 
Blackjack is a popular card game played in casinos worldwide. 
In 1962 the mathematician Edward Thorb published a Book on how to beat the house in Blackjack \cite{b1}. 
This made is possible for players to win money in the game of Blackjack in casinos. 
Although the casinos changed their rules to prevent the player from winning in the game of blackjack impossible now, the game is still an interesting subject to mathematical modeling due to its probabilistic nature.    

Edward Thorb introduced the idea of card counting with the Complete Point Count System \cite{b1} which makes it possible for players to keep track of a score while playing that indicates their chances of winning.
The Complete Point Count System keeps track of a counter that adds +1 or -1 based on each card that are seen by the player. 
This easy counting method makes it possible for players to count cards while playing the game. 

\subsection{Reinforcement Learning for Blackjack}
The game can be modeled as a Markov Decision Process which makes it possible to apply Reinforcement Learning.
With todays computation power it is now possible to let the computer play over 100.000.000 games, which wasn't possible in the 1960s. 
The baseline goal is therefore to at least achieve the basic strategy of Edward Thorb \cite{b1} with Reinforcement Learning methods.

All of the Reinforcement Learning methods play a large number of games while keeping track of a Q-Table containing the expected returns to each of the possible game states \cite{?}.
The different Reinforcement Learning methods like Sarsa, Q-Learning and Temporal Difference only take a different approach of updating this Q-Table \cite{?}.
Although the Reinforcement Learning methods are very powerful, they require an increasing amount of games to be able to model the Q-Table as the state-space of the game increases. 
In this paper we therefore investigate how well the Reinforcement Learning methods Sarsa, Q-Learning and Temporal Difference Learning perform as we increase the state-space by changing the card counting method. 
We change the card counting method by keeping track of the values of every seen card and not just keeping track of a counter. 

\subsection{Deep Reinforcement Learning}
In 2013 DeepMind published the paper \textit{Playing Atari with Deep Reinforcement Learning} \cite{b2}, combining Deep Learning to Reinforcement Learning.
The idea of Deep Reinforcement Learning is to replace the Q-Table with a deep neural network.
This makes it possible to estimate the expected returns even for complex state-spaces.

In this paper we first investigate whether Deep RL is also able to achieve the baseline of the basic strategy of Edward Thorb \cite{b1}.
Secondly we compare how the Deep RL performs compared to the more traditional RL-methods like Sarsa, Q-Learning and Temporal Difference Learning.

% TODO: Do I have the structure of: Establishing niche and occupying niche?


\section{Methods}

\subsection{The Blackjack Environment}
The game of Blackjack start by the dealer and the player recieving two cards where the player is only allowed to see one of the dealer's cards and his own cards
The player only has the two actions of taking another card (Hitting) and not taking another card (Standing). 
Although the real rules of Blackjack extend this action-space by options like Doubling Down and Splitting, in this paper we only focus on Standing and Hitting. 
The Blackjack Environment used for the implementation of this paper only focuses on Hitting and Standing. 

The player can then hit as many as he wants with the goal of coming as close to the cards value of 21 as possible.
After the player finished his turn the dealer hits as long as his cards value are below or equal 16.
At the end of one game the player's return is +1, -1 or 0 based on whether it was a win, loss or draw.

To model the basic strategy with Reinforcement learning, one game like that can be seen as one episode. 
This is possible since the basic strategy only focuses on whether the player should Hit or Stand based on the dealer's card value and the player's cards value.  

When introducing card counting, one episode has to be extended by playing multiple games in one episode. 
This is necessary since card counting is dependent on seeing a high number played cards since the player's chances change depending on the ratio of low cards/ high cards played. 
In the case of one game the player on average see 3-5 cards which is not enough for an imbalanced ratio between high and low cards.
Therefore one episode is extended to playing through one whole deck.

The Complete Point Count System \cite{b1} keeps a running score and updating it every time the player sees a card:
\begin{itemize}
	\item +1 for: 2, 3, 4, 5, 6, 7
	\item -1 for: 10, A
	\item 0 for: 8, 9 
\end{itemize}
It is favourable for the player when the running score is high \cite{b1}.
And it is favourable for the dealer when the running score is low.
This is because the dealer's chances of busting increases when there are only few low cards left in the deck.

\subsection{State-action space}
For learning the basic strategy the state consists of the \textbf{player's cards value}, the \textbf{dealer's card value} and if the player has an \textbf{usable ace}. 
This makes a total number of \textbf{250 states} that need to be considered:
\begin{itemize}
	\item 10 possible dealer states: 2,3,4,5,6,7,8,9,10,A
	\item 25 possible player states:
		\subitem No Ace: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20
		\subitem With Ace: 13,14,15,16,17,18,19,20
\end{itemize}
% player: 2 bis 21 = 2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20,21 = 19 

For learning the Complete Point Count System the state has to be extended by \textbf{running score}.
The running score in one game can at most be 24 (= 6 cards * 4 suits) and at least -20 (= 5 cards * 4 suits).
This would increase the state-space to \textbf{11.000 states} (= 250 states * (24 - 20)) for the Complete Point Count System, although the running score around 0 are far more likely than 24 or -20.

And when making the card counting method more complex by counting every value of the seen cards, the state space increases to approximately \textbf{907.000.000 states} (= 250 states * ((52 cards / 4 suits) - 3 face cards)! ) for playing through one card deck.  
Altough the actual state-space is a little less considering that the player can only see one card of the dealer.
The most important thing to note here is that the state-space explodes when using the more complex card counting method. 
% TODO: Is this a correct calculation?

With this huge state space it is interesting to observe how Deep Reinforcement Learning performs and how other Reinforcement Learning methods perform compared to that. 


\subsection{Reinforcement Learning methods}
All of the Reinforcement Learning methods share the basic approach of taking actions in an environment, observing the reward and keeping track of the expected reward to each state-action \cite{b4}.
One more thing that all of the RL methods have in common is that they to play a large number of episodes to be able to estimate the expected rewards well.
% TODO: They share Bellman Equation, right?

The implementation of RL methods Monte Carlo Control, SARSA and Q-Learning mainly differ on the following characteristics \cite{b4}:  
\begin{itemize}
	\item \textbf{On-policy vs. Off-policy Learning}: If the same policy is used for exploration and exploitation steps. 
	\item \textbf{Update Rule}: With what information the updates are done. 
	\item \textbf{Bootstrapping}: If estimations are done based on prediction of future values. 
\end{itemize}

These are the terminologies used for the rest of this paper: 
\begin{itemize}
	\item Q: action-value function
	\item V: state-value function
	\item G: cumulative reward after state is visited until the end of the episode 
	\item R: reward
	\item S: state
	\item A: action
	\item t: timestep
	\item $\gamma$: discount rate
	\item $\alpha$: learning rate
\end{itemize}

\subsubsection{Monte Carlo Control}
Monte Carlo Control (MC) was also introduced in the book \textit{Reinforcement learning: An introduction} \cite{b4}.
MC is an on-policy learning method which uses the epsilon-greedy method for deciding on whether to take a exploration or an exploitation step.
This the update rule of Monte Carlo Control:
\begin{equation*}
	V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)] \tag{1}
\end{equation*}
MC does not use Bootstrapping which means that the updates are one calculated based on real observations. 
This property makes Monte Carlo Control converge slower \cite{b4}.  

\subsubsection{SARSA}
The State-Action-Reward-State-Action (SARSA) method was introduced in the book \textit{Reinforcement learning: An introduction} \cite{b4} and counts as a Temporal Difference Learning method.
SARSA is also an on-policy learning method which means that the same policy is used for exploration and exploitation steps.  
For the update rule SARSA takes the reward of the next state in consideration:
\begin{equation*}
	Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] \tag{2}
\end{equation*}
This means that SARSA uses bootstrapping since $Q(S_{t+1})$ is used for updating $Q(S_t, A_t)$. 

\subsubsection{Q-Learning}
Q-learning first introduced by Watkins \cite{b5} and taken up in \cite{b4} also counts as Temporal Difference Learning method.
Q-learning is an off-policy learning method \cite{b4}.
\begin{equation*}
	Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)] \tag{3}
\end{equation*}

Although on-policy learning has its advantages, for the case of Blackjack the off-policy learning is expected to perform better since the target policy does not have to do exploration steps which lower the expected reward.


\subsection{Deep Q-Learning}
Deep Q-Learning (DQN) originated in 2013 in the DeepMind paper \textit{Playing Atari with Deep Reinforcement Learning} \cite{b2}.
The main idea of DQN to replace the Q-Table with a deep neural network \cite{b2}. 
Further, two important implementation details are the usage of an Experience Replay Buffer and a Target Neural Network which are presented in the following section.

The pseudo-code for the DQN can be inspected in the \textit{Playing Atari with Deep Reinforcement Learning} paper \cite{b2}.
For a deeper explanation of the DQN architecture is \textit{Implementing the Deep Q-Network} \cite{b6} a helpful resource. 

\subsubsection{Experience Replay Buffer} \label{replay-buffer}
% TODO: Experience Replay was already there in Atari paper
% TODO: Absatz zu Replay Buffer
At each step of the training loop the Experience Replay Buffer is filled with the Experience of this training step. 
An Experience consists of:
\begin{itemize}
	\item state
	\item action
	\item reward
	\item next state
\end{itemize}
At every training step a random mini-batch of the Buffer is then sampled and used for the training of the neural network.

The Experience Buffer has the advantage of reusing Experiences for training the neural network which increases the data effiency \cite{b2}.
Additionally, sampling random mini-batches uniformly introduces the advantage of breaking the dependency of a state with its preceeding state which achieves a more general neural network \cite{b2}. 

The maximum size of the Buffer is set to 10.000.
When the maximum size is reached, the oldest Experiences are removed and for new Experiences to be added. 

\subsubsection{Target Neural Network} \label{target-network}
% TODO: Absatz zu target network (From paper)
In traditional Q-Learning the current Q-value is updated by the estimated Q-value of the next state.
To facilitate the same update rule in Deep Q-Learning a Target Neural Network, called $Q^*$ in the DeepMind paper \cite{b2}, is used for predicting the returns of the next state \cite{b6}.
These predicted returns of the next state are then used for the loss function and ultimately for the gradient descent of the learning neural network \cite{b6}. 

The weights of the Target Neural Network are fixed and only get updated by the weights of the training network. 
In the paper \cite{b6} this update of the weights is done only every few training steps with the advantage of faster training time. 
In the implementation for this paper, the updates are done every training step but with a update rate of only 0.005 to prevent overfitting.

The advantages of using the target neural network are more stable training and that the errors in estimation are better controlled \cite{b6}.



\section{Results}

\subsection{Basic Strategy}
% TODO: Basic Strategy von allen 4 Methoden erreicht. Wie möchte ich das zeigen? -> Policy?

\subsection{Increased state space - counting all cards}
% TODO: Counting all cards (sollte die state-space erheblich erhöhen). Wie verhalten sich die 3 traditionellen im Vergleich zu DQN?
% TODO: Plots: Vllt. Value functions? -> Es ist schwierig das darzustellen.
% TODO: Plots: Vllt. T-SNE plot?

\section{Discussion}
% TODO: ? Haben die Methoden so gut abgeschlossen, wie erwartet? 
% TODO: . Wie haben sich die Methoden im Vergleich verhalten? Wie lässt sich das erklären?
% TODO: . Was bedeuten diese Ergebnisse? -> Dass DQN für high state-space eingesetzt werden sollte. Dass die anderen Methoden aber auch gut sind, wenn der state-space nicht soo hoch ist. 
% TODO: . ? Was noch?  


\section{Conclusion}
% TODO: Es wäre interessant: 
% . Mit Geld spielen. Ob es DQN schafft einen höheren Expected Reward zu erreichen als traditionelle Methoden 
% . Verhalten von DQN ohne Verbesserungen (Replay Buffer und Target network)
% . Ob sich das trainierte DQN einfach auf andere (ähnliche) Kartenspiele, wo es ums Punkte sammeln geht, anwenden lässt - Ohne eine Veränderung machen zu müssen.
	% . Das haben sie früher bei Atari auch ausprobiert und es ist sehr interessant gewesen.



%\begin{table}[htbp]
%\caption{Table Type Styles}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%\cline{2-4} 
%\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%\hline
%copy& More table copy$^{\mathrm{a}}$& &  \\
%\hline
%\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%\end{tabular}
%\label{tab1}
%\end{center}
%\end{table}

%\begin{figure}[htbp]
%\centerline{\includegraphics{fig1.png}}
%\caption{Example of a figure caption.}
%\label{fig}
%\end{figure}

\begin{thebibliography}{00}
\bibitem{b1} Thorp, E. O. (1966). Beat the dealer: A winning strategy for the game of twenty-one (Vol. 310). Vintage.
\bibitem{b2} Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., \& Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.
\bibitem{b3} Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... \& Hassabis, D. (2015). Human-level control through deep reinforcement learning. nature, 518(7540), 529-533.
\bibitem{b4} Sutton, R. S., \& Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
\bibitem{b5} Watkins, C. J., \& Dayan, P. (1992). Q-learning. Machine learning, 8, 279-292.
\bibitem{b6} Roderick, M., MacGlashan, J., \& Tellex, S. (2017). Implementing the deep q-network. arXiv preprint arXiv:1711.07478.
\end{thebibliography}
\end{document}
